# ðŸ”¬ Ternary Network Quantization: Comprehensive Comparison
*A comparative study of ternary weight quantization methods for neural network compression*

**Universitatea BabeÈ™-Bolyai Cluj - Neural Networks Project**

## ðŸŽ¯ Project Overview

This repository implements and compares four neural network quantization approaches on MNIST using LeNet-5:

- **FP (Full Precision)**: 32-bit floating-point baseline
- **TWN (Ternary Weight Networks)**: Symmetric ternary quantization {-Î±, 0, +Î±}
- **TTQ (Trained Ternary Quantization)**: Asymmetric ternary quantization {-Î±â», 0, +Î±âº}
- **ADMM**: Constrained optimization approach with dual variables

## ðŸ† Key Results

![Accuracy vs Compression Comparison](results_comparison.png)

| Method | Test Accuracy | vs Baseline | Theoretical Compression |
|--------|---------------|-------------|------------------------|
| **FP** | **98.96%** | Baseline | 1Ã— |
| **ADMM** | **98.67%** | **-0.29%** | **16Ã—** â­ |
| **TWN** | **95.46%** | -3.50% | 16Ã— |
| **TTQ** | **94.60%** | -4.36% | 16Ã— |

**ðŸŽ¯ Key Finding**: ADMM achieves near-baseline accuracy (98.67%) while maintaining 16Ã— compression through principled constrained optimization.

## ðŸ“ Repository Structure

```
ternary networks ubb/
â”œâ”€â”€ train.py              # Main training script (4 modes: fp/twn/ttq/admm)
â”œâ”€â”€ eval.py               # Model evaluation and weight counting
â”œâ”€â”€ tune_hyperparams.py   # Systematic hyperparameter optimization
â”œâ”€â”€ plot_results.py       # Results visualization generator
â”œâ”€â”€ deployment_compression.py  # True deployment size analysis
â”œâ”€â”€ report.md             # Detailed technical report
â”œâ”€â”€ README.md             # This documentation
â”œâ”€â”€ checkpoints/          # Trained models (.pth files)
â”‚   â”œâ”€â”€ fp.pth           # Full precision model
â”‚   â”œâ”€â”€ twn.pth          # TWN model
â”‚   â”œâ”€â”€ ttq.pth          # TTQ model
â”‚   â””â”€â”€ admm.pth         # ADMM model
â”œâ”€â”€ results.csv           # Final evaluation results
â”œâ”€â”€ results_comparison.png # Results visualization
â”œâ”€â”€ tuning_results/       # Hyperparameter search results
â”‚   â”œâ”€â”€ admm_tuning.json
â”‚   â””â”€â”€ tuning_summary.json
â”œâ”€â”€ data/                 # MNIST dataset (auto-downloaded)
â”œâ”€â”€ ternary_env/          # Python virtual environment
â”œâ”€â”€ ternary-networks.pdf  # Project handout
â”œâ”€â”€ TWN.pdf               # TWN research paper
â””â”€â”€ TTQ.pdf               # TTQ research paper
```

## ðŸš€ Quick Start

### 1. Environment Setup
```bash
# Create and activate virtual environment
python3 -m venv ternary_env
source ternary_env/bin/activate  # Linux/Mac
# ternary_env\Scripts\activate   # Windows

# Install dependencies
pip install --upgrade pip
pip install torch torchvision matplotlib pandas
```

### 2. Train All Models
```bash
# Train all quantization methods (3 epochs each)
python train.py --mode fp     # Full precision baseline
python train.py --mode twn    # Ternary Weight Networks
python train.py --mode ttq    # Trained Ternary Quantization
python train.py --mode admm   # ADMM optimization
```

### 3. Evaluate and Visualize
```bash
# Evaluate all models and generate results.csv
python eval.py --mode fp
python eval.py --mode twn
python eval.py --mode ttq
python eval.py --mode admm

# Generate visualization
python plot_results.py

# Analyze deployment compression
python deployment_compression.py
```

## ðŸ”¬ Method Implementations

### TWN (Ternary Weight Networks)
```python
# Symmetric quantization with threshold-based approach
threshold = 0.75 * torch.mean(torch.abs(weight))
mask = torch.abs(weight) > threshold
alpha = torch.mean(torch.abs(weight[mask]))
ternary_weight = torch.where(weight > threshold, alpha,
                           torch.where(weight < -threshold, -alpha, 0))
```
**Features**: Simple, symmetric, single Î± per layer

### TTQ (Trained Ternary Quantization)
```python
# Asymmetric quantization with separate positive/negative scaling
normalized_weight = weight / torch.max(torch.abs(weight))
ternary_weight = torch.where(intermediate > 0, self.alpha_p,
                           torch.where(intermediate < 0, -self.alpha_n, 0))
```
**Features**: Separate Î±âº/Î±â» parameters, better weight distribution

### ADMM (Alternating Direction Method of Multipliers)
```python
# Constrained optimization with dual variables
ternary_weight = ternary_projection(weight + dual_variable)
dual_variable += rho * (weight - ternary_weight)
```
**Features**: Principled optimization, dual variables, stable convergence

## ðŸ“Š Detailed Analysis

### Training Configuration
- **Architecture**: LeNet-5 (Conv: 1â†’32â†’64, FC: 1024â†’512â†’10)
- **Dataset**: MNIST (60k train, 10k test)
- **Optimizer**: Adam with method-specific learning rates
- **Epochs**: 3 (sufficient for MNIST convergence)

### Hyperparameter Optimization
- **Systematic grid search** over learning rates, batch sizes, and ADMM penalty parameters
- **Best configurations**:
  - FP/TWN: lr=0.001, batch_size=64
  - TTQ: lr=0.0005, batch_size=64
  - ADMM: lr=0.002, batch_size=128, rho=1e-5

### Compression Analysis
**Training vs Deployment Reality**:
- Training checkpoints: All ~2.3MB (store optimization states)
- True deployment: FP=2,271KB, Ternary=142KB each
- **16Ã— compression** achieved through 32-bit â†’ 2-bit weight encoding

## ðŸ” Advanced Usage

### Hyperparameter Tuning
```bash
# Tune specific method
python tune_hyperparams.py --mode admm

# Tune all methods
python tune_hyperparams.py --mode all

# View tuning results
cat tuning_results/tuning_summary.json
```

### Custom Training
```bash
# Custom learning rate and batch size
python train.py --mode admm --lr 0.001 --batch_size 128 --epochs 5

# Enable hyperparameter tuning during training
python train.py --mode ttq --tune
```

### Evaluation Options
```bash
# Evaluate specific checkpoint
python eval.py --mode admm --checkpoint ./checkpoints/admm.pth

# Count and analyze weight sparsity
python eval.py --mode twn  # Shows layer-wise weight statistics
```

## ðŸŽ¯ Key Insights

### Why ADMM Outperforms TWN/TTQ
1. **Principled optimization**: Constrained optimization framework vs direct quantization
2. **Dual variables**: Better gradient flow and convergence stability
3. **Conservative penalty terms**: Prevents optimization instability
4. **Gradual transition**: Smooth continuousâ†’ternary weight evolution

### Compression Reality
- **Training files**: Store full optimization states (no apparent compression)
- **Deployment files**: True 16Ã— compression through 2-bit encoding
- **BatchNorm impact**: Still requires full precision in practice

### Technical Breakthroughs
- **ADMM convergence fix**: Initial 10% â†’ final 98.67% accuracy
- **Dual variable stabilization**: Clipping prevents gradient explosion
- **Adaptive thresholding**: Percentile-based vs fixed thresholds

## ðŸ“š Documentation

- **`report.md`**: Comprehensive technical analysis
- **Papers**: `TWN.pdf`, `TTQ.pdf`, `ternary-networks.pdf`

## âš™ï¸ System Requirements

- **Python**: 3.8+
- **PyTorch**: 1.9+
- **Memory**: ~2GB for training, ~100MB for inference
- **Storage**: ~500MB for all checkpoints and data

## ðŸš¨ Troubleshooting

### Common Issues
```bash
# ADMM training instability
python train.py --mode admm --rho 1e-6  # Lower penalty parameter

# CUDA out of memory
python train.py --mode ttq --batch_size 32  # Smaller batch size

# Dependency issues
pip install --upgrade torch torchvision  # Update PyTorch
```

### Validation
```bash
# Verify installation
python -c "import torch; print(f'PyTorch {torch.__version__} ready')"

# Test basic functionality
python train.py --mode fp --epochs 1  # Quick validation run
```

## ðŸ”® Future Directions

1. **Deployment optimization**: True sparsity implementation
2. **Larger datasets**: CIFAR-10, ImageNet experiments
3. **Hardware acceleration**: Custom CUDA kernels for ternary operations
4. **BatchNorm fusion**: Complete ternary deployment pipeline

## ðŸ¤ Contributing

This is an educational project demonstrating ternary quantization methods. Feel free to:
- Experiment with different architectures
- Extend to other datasets
- Improve ADMM optimization
- Add new quantization methods

---

**ðŸŽ¯ Project Highlight**: Our ADMM implementation achieves remarkable 98.67% accuracy (only -0.29% vs full precision) while maintaining 16Ã— theoretical compression, demonstrating the superiority of principled optimization approaches for neural network quantization. 